Agents

While the Enterprise Search and summarisation features in Agentspace deliver significant value to an organisation through enhanced workforce efficiency and productivity (and the associated cost savings), the platform unlocks its transformative potential with the introduction of agents. These are specialised AI assistants, powered by Google's state-of-the-art Large Language Models like Gemini, engineered to move beyond simple information access towards proactive task execution and intelligent workflow automation. Agents excel at tackling complex, multi-step challenges, orchestrating actions, and integrating capabilities across various enterprise systems. They achieve this by combining information retrieval with advanced reasoning and the ability to execute defined actions, effectively automating entire processes, driving deeper efficiency gains and enabling new levels of operational capability.

Out of the box, Agentspace features Google’s DeepResearch agent, designed to perform comprehensive analysis on a given topic. Its 'deep analysis' involves synthesising information from diverse datasources (including configured enterprise data sources and the public web) to generate novel insights and structured understanding that go beyond simple keyword searches. For instance, instead of just getting a list of search results, imagine asking it to investigate 'customer reactions to our latest product update.' DeepResearch could analyze feedback from your customer support system, comments on your community forum, and public social media posts. It would then deliver a synthesized report highlighting:

The top 3 things customers love about the update.
The 2 most common complaints (e.g., "the new interface is slower on older devices" or "feature X is harder to find now").
Perhaps even an unexpected insight, such as "many users are asking for a tutorial on a specific new function," or "customers in the UK are responding more positively than those in the US."

This kind of structured summary, with findings clearly identified and traced back to the original information sources, is far more actionable than raw search output.

A defining characteristic of the DeepResearch agent is its emphasis on user control for complex tasks: it first presents a detailed research plan outlining its intended steps, potential sources, and expected lines of inquiry. The user reviews and approves this plan before the agent proceeds with the execution. This human-in-the-loop (HITL) mechanism ensures the research aligns with the user's intent and allows for oversight before extensive processing occurs. Upon approval, the agent executes the plan, returning a detailed, structured report of its findings.

<insert image of DeepResearch agent – ideally showcasing its planning or results interface>

Whilst DeepResearch currently appears to be the primary Google-provided agent OotB, the Agentspace UI is clearly structured to accommodate a growing ecosystem of agents. It features dedicated sections for preconfigured 'Google Agents', 'From Your Organisation' agents (custom agents shared within the company), and 'Your Agents' (personal custom agents). This design strongly suggests Google's intention to release additional preconfigured agents in the future.

<insert screenshot of UI – ensure this clearly highlights the agent categories: 'Google Agents', 'From Your Organisation', and 'Your Agents'. Light annotations on the screenshot could further clarify these sections.>

Custom Agents

Custom agents truly embody the principles of agentic AI, offering maximum value to enterprises by allowing the tailoring of AI capabilities to specific business needs and processes. It's likely that many unique organisational workflows will require custom agents to effectively handle their specific nuances. These agents are powerful because they are grounded in your enterprise data sources and leverage company specific tools and actions alongside their core reasoning abilities to achieve defined goals.

Agentspace provides a 'no-code' interface for creating these custom agents, empowering individuals who deeply understand business processes, yet may lack traditional programming backgrounds, to build their own. Custom agents within Agentspace are created through a few simple steps, primarily using natural language prompts:

- Define a name for the agent: A clear, descriptive name helps identify its purpose.
- Define a goal for the agent: This concisely explains what the agent is designed to achieve.
- Define instructions for the agent: This is where you detail the agent's scope, capabilities, objectives, and the methods it should use. You can specify response formatting, conditions for its actions, and the tools or actions it should call.

Crafting these instructions is key to creating an effective agent. It relies on principles similar to prompt engineering: instructions must be specific, unambiguous, and provide sufficient context. Defining the perfect instructions is an iterative process. Expect to test, observe the agent's behaviour, and refine your prompts to achieve optimal performance. To assist with this, after drafting initial instructions, Agentspace includes a handy 'help me write' button. This feature leverages AI to analyze your input and suggest improvements for clarity, completeness, and efficiency, often incorporating best practices in prompt engineering to make your agent more effective. This AI-assisted instruction refinement is a form of meta-prompting, where Generative AI helps you craft even more descriptive and potent instructions from your initial ideas.

Configure Available Tools/Actions: Specify the capabilities the agent can leverage to interact with the outside world or perform specific functions. This goes beyond just invoking predefined 'Actions' within Agentspace. You can grant the agent access to a variety of tools, such as calling external REST APIs (e.g., to update a customer record in your CRM or fetch real-time shipping information for an order), triggering Cloud Functions for custom backend logic, utilising specialised Vertex AI Tools like Vertex AI Search for advanced retrieval-augmented generation (RAG) across your documents, Vertex AI Vision for image analysis tasks (such as reading a product label from an uploaded photo), or the Natural Language API for sentiment analysis within a custom tool workflow. You can also enable interaction with databases via configured connectors, or the execution of other custom-built functions.

Under the hood, this relies on the underlying Large Language Model's ability for tool use (often referred to as function calling). The LLM uses its advanced reasoning capabilities to understand the user's intent and the agent's overall goal. It then intelligently determines if and when to employ a specific tool from its available toolkit, and crucially, how to generate the correct parameters for that tool based on the ongoing conversational context and its ultimate objective.

Link Data Sources for Grounding: Connect the agent to relevant enterprise knowledge using the Connectors you've configured (as discussed in the previous post on Connectors and Datasources [Optional: Hyperlink to previous post here]). This step is crucial for grounding the agent, ensuring its responses, reasoning, and actions are based on factual, up-to-date company information rather than solely relying on the LLM's general pretrained knowledge, thereby minimizing hallucinations, improving accuracy, and building user trust in the agent's outputs. This effectively leverages the principles of Retrieval-Augmented Generation (RAG), providing specific, timely context pulled from your data sources to inform the agent's task execution.

Once created, interaction with custom agents occurs within the unified Agentspace UI, where they appear alongside other features, often under a dedicated menu section. Users engage with the agent using natural conversation through the chat interface to initiate tasks and receive assistance.

The availability of a no-code agent design approach provides many benefits including:

Accessibility & Empowerment: It significantly lowers the barrier to entry, empowering non-technical users who are closest to the business challenges to directly design and implement AI solutions tailored to their specific needs. They no longer need to solely rely on specialist development teams, reducing bottlenecks.
Speed & Agility: Development cycles are dramatically accelerated. Building, testing, and iterating on custom agents can be done much more rapidly when visual tools replace complex coding, allowing organisations to respond quickly to changing business requirements.
Focus on Business Logic: Users can concentrate on what they want the agent to achieve and the business rules it should follow, rather than getting bogged down in the how of technical implementation. The interface abstracts away the underlying complexity.
Reduced Development Costs: While not eliminating the need for technical oversight entirely, it can reduce the direct hours and reliance on specialised AI developers for many common agent creation tasks.
Increased Innovation: By enabling a broader base of employees to experiment and build, organisations can foster a culture of innovation, uncovering new ways to automate processes and improve efficiency that might not have been identified by a central IT team alone.

This no-code interface shifts agent creation from a complex technical task to a more accessible business capability, allowing for faster delivery of custom AI solutions that directly meet enterprise requirements.

Agent Reasoning and Orchestration

Beyond simply executing individual commands based on their instructions and tools, a core strength of Agentspace agents lies in their reasoning and orchestration capabilities. When presented with a complex goal, agents don't just perform a single action. Instead, powered by their underlying Large Language Model, they intelligently break down the request into smaller, logical steps. Many agents employ methodologies conceptually similar to ReAct (Reason + Act), where they internally 'think' about the current state and goal, decide on the next best action (e.g., Thought: 'The user wants to book a flight. First, I need to know their destination and dates. I should ask for that.' Action: Ask user for destination and dates.), execute that action, observe the outcome (e.g., Observation: 'User provided destination: London, Dates: July 10-15.'), and then repeat the reasoning process to determine the subsequent step (e.g., Thought: 'Now I have the destination and dates, I need to check flight availability. I should use the Flight Search tool.' Action: Call Flight Search tool with parameters London, July 10-15.).

This iterative cycle of reasoning, acting, and observing allows agents to dynamically navigate complex, multi-step workflows. The explicit 'plan' presented for approval by the DeepResearch agent is a visible example of this sophisticated planning capability in action, providing transparency for intricate tasks. Custom agents utilize similar, though often internal and implicit, reasoning processes to orchestrate the sequence of operations needed to achieve their specific objectives. This means they might not always present an explicit plan for pre-approval like DeepResearch (unless specifically designed to incorporate such checkpoints for user validation), but the underlying LLM still performs this crucial multi-step reasoning to sequence actions effectively. This could involve gathering information from multiple sources, making sequential decisions based on evolving data, or interacting with several different systems to complete a task like processing an expense claim (as detailed in the example below).

Agent Security
As these custom agents interact with enterprise systems and execute tasks, ensuring robust security is naturally a paramount concern. Agentspace is built upon a clear, user-centric security model: all actions performed by an agent are executed under the delegated authority of the logged-in user. This means that when an agent, for instance, retrieves data from a proprietary database or triggers an action in another company application as part of its automated workflow, it does so using the user's existing authenticated session. Consequently, the agent is inherently bound by that individual's specific permissions and access controls within those target systems. This approach not only provides a transparent audit trail, as all operations can be clearly traced back to the initiating user, but also underscores the critical importance of applying the principle of least privilege to user permissions across all connected applications. Furthermore, for any tools integrated with an agent that might require their own distinct credentials (such as API keys for third-party SaaS platforms), these must always be managed securely, integrating with dedicated services like Google Secret Manager to prevent direct exposure and uphold rigorous credential management standards

Agent Example Use Case

An example agent we can conceptualise involves simplifying expense claim submissions. Depending upon the system and the user's experience submitting expense claims, this can be a time-consuming and challenging process. For users that don’t frequently submit expenses, they often need to find documentation or a colleague to guide them through the process (something that could otherwise be done via Enterprise Search in Agentspace with a simple KB article). Once they have identified the system, they then need to authenticate and then navigate the UI, submitting and classifying the expense claim, whilst trying to ensure adherence to corporate policies.

With a custom agent, this process can be significantly simplified and managed from within the Agentspace UI.

- The user uploads an image of a receipt they want to claim expenses for to the 'Expenses Agent.'
- The agent identifies the uploaded file as an image and processes it as a receipt, leveraging the built-in multi-modal capabilities of Agentspace, powered by Gemini, for image understanding and initial data extraction from the receipt.
- It then extracts key information like the date, vendor, items, and cost. These details are used to classify the expense type, potentially by comparing item descriptions against a company's predefined expense categories or using the LLM's reasoning for classification based on learned patterns.
- The agent also checks if the expense aligns with the defined company expense policy in real time.
- The agent then submits the extracted data alongside additional information it knows about the user (e.g., name, email address, department), which it retrieves from the user's Agentspace profile or by querying an internal employee directory via a connected tool.
- It prompts the user within the Agentspace chat interface to validate the detected information, add any missing mandatory data not stored in the user profile or on the receipt (e.g., cost centers, project codes), and confirm compliance with policy if any flags were raised.
- The agent then transfers the validated data into a structured format like JSON, prior to submitting the expense request to the corresponding backend system, for example, by using a custom tool that calls the company's expense management system's REST API.
- Finally, the agent emails confirmation of the expense claim submission to the user, providing a record of the transaction alongside links to documentation for expense FAQs.

This provides a better customer experience for the person submitting the expense claim, improves efficiency as the entire process is managed and guided by the agent who is available 24/7, ensures improved compliance with expense policies through proactive checks, and caters for validation and correct submission of expense claims, reducing the likelihood of user error due to inexperience.

Where Next?

Agentspace already delivers substantial value with its current suite of features for Enterprise Search, Summarization, and custom agent-driven process automation. However, the field of agentic AI is evolving rapidly, and looking ahead, a couple of key developments could unlock even more transformative potential within the platform:

- Advancing Agent-to-Agent Collaboration:
The recent buzz around Google's Agent-to-Agent (A2A) communication protocol (announced at Google Cloud Next '25) points to an exciting future. Currently, building very complex workflows in Agentspace can sometimes lead to highly intricate instructions for a single agent managing multiple sub-tasks. Enhanced native A2A capabilities within Agentspace would allow for the development of more modular, specialised agents that can collaborate seamlessly. This aligns with agentic AI design principles, where smaller, focused agents are often easier to develop, manage, maintain, and scale.

- Broadening Integration and Interoperability:
The ability to easily integrate or import agents developed outside of the core Agentspace environment would be another significant leap forward. If agents built with Google's Agent Development Kit (ADK), or agents running on Vertex AI Agent Engine (or other open source frameworks) could be seamlessly brought into Agentspace it would offer a powerful solution to orchestrating diverse, specialised agents (potentially addressing the first point as well), but would also dramatically expand the accessible toolkit for enterprises, unleashing further innovation.

While these areas represent exciting frontiers for growth, Google's deep commitment to pioneering AI and agentic technologies paints a promising picture for the continued evolution of Agentspace. The journey towards intelligent, autonomous enterprise operations is accelerating, and platforms like Agentspace are pivotal in shaping that future.
